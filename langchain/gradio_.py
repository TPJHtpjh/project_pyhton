import gradio as gr
from langchain_community.llms import Ollama  # ä½¿ç”¨æ­£ç¡®çš„å¯¼å…¥
from langchain.memory import ConversationBufferMemory
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.chains import ConversationChain

# å…¨å±€å†…å­˜å¯¹è±¡ - ç¡®ä¿å¯¹è¯å†å²æŒä¹…åŒ–
memory = ConversationBufferMemory(
    return_messages=True,
    memory_key="chat_history",
    ai_prefix="AIåŠ©æ‰‹",
    human_prefix="ç”¨æˆ·"
)

def create_chain(temperature, max_length, personality):
    """åˆ›å»ºå¸¦å‚æ•°çš„å¯¹è¯é“¾"""
    # 1. åˆ›å»ºæ¨¡å‹å®ä¾‹
    llm = Ollama(
        model='deepseek-r1:7b',
        base_url='http://localhost:8080',
        temperature=temperature,
        num_predict=max_length  # ä½¿ç”¨æ­£ç¡®çš„å‚æ•°å
    )
    
    # 2. åˆ›å»ºæç¤ºæ¨¡æ¿
    prompt = ChatPromptTemplate.from_messages([
        ("system", f"ä½ ç°åœ¨çš„äººç‰©è®¾å®šï¼š{personality}ã€‚"),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{input}")
    ])
    
    # 3. åˆ›å»ºå¯¹è¯é“¾
    return ConversationChain(
        llm=llm,
        memory=memory,
        prompt=prompt,
        verbose=True
    )
def clean_response(response):
    if r"<think>" in response:
        response=response.split(r"<think>")[-1]
        response=response.split(r"</think>")[-1]
    # # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
    # cleaned = response.replace('\n', '').replace('\r', '')
    return response
def respond(message, chat_history, temperature, max_length, personality):
    """å¤„ç†ç”¨æˆ·æ¶ˆæ¯å¹¶è¿”å›å“åº”"""
    # åˆ›å»ºå¸¦å‚æ•°çš„å¯¹è¯é“¾
    chain = create_chain(temperature, max_length, personality)
    
    # è°ƒç”¨å¯¹è¯é“¾ - ä½¿ç”¨æ­£ç¡®çš„è¾“å…¥æ ¼å¼
    response = chain.invoke({"input": message})["response"]
    response=clean_response(response)
    # æ›´æ–°èŠå¤©å†å²
    chat_history.append((message, response))
    
    return "", chat_history

def clear_memory():
    """æ¸…é™¤å†…å­˜å¹¶è¿”å›ç©ºèŠå¤©å†å²"""
    memory.clear()
    return []
def process_voice(audio_data):
    """å¤„ç†è¯­éŸ³è¾“å…¥å¹¶è¿”å›æ–‡æœ¬"""
    if audio_data is None:
        return ""
    try:
        import speech_recognition as sr
        recognizer = sr.Recognizer()
        audio = sr.AudioData(voice.tobytes(), sample_rate=44100, frame_width=2)
        text = recognizer.recognize_google(audio, language='zh-CN')
        return text
    except Exception as e:
        return f"è¯­éŸ³è¯†åˆ«é”™è¯¯: {str(e)}"
def save_chat(chat_history):
    if not chat_history:
        return "å°šæœªå¯¹è¯"
    try:
        with open("chat_history.txt", "w", encoding="utf-8") as file:
            for human, ai in chat_history:
                file.write(f"ç”¨æˆ·: {human}\nAIåŠ©æ‰‹: {ai}\n\n")
        return "å¯¹è¯å·²ä¿å­˜"
    except Exception as e:
        return f"ä¿å­˜å¯¹è¯æ—¶å‡ºé”™: {str(e)}"

with gr.Blocks(theme=gr.themes.Soft()) as demo:
    gr.Markdown("# ğŸ¤– AI èŠå¤©æœºå™¨äºº")
    
    # èŠå¤©ç•Œé¢
    chatbot = gr.Chatbot(label="å¯¹è¯å†å²", height=400)
    msg = gr.Textbox(label="è¾“å…¥æ¶ˆæ¯", placeholder="åœ¨è¿™é‡Œè¾“å…¥æ‚¨çš„é—®é¢˜...")
    
    # è®¾ç½®é¢æ¿
    with gr.Accordion("é«˜çº§è®¾ç½®", open=False):
        temperature = gr.Slider(0.1, 1.0, value=0.7, label="åˆ›æ„åº¦", interactive=True)
        max_length = gr.Slider(250, 1000, value=100, step=10, label="å›å¤é•¿åº¦", interactive=True)
        personality = gr.Dropdown(
            ["æ¸©æŸ”çŸ¥æ€§", "çƒ­è¡€ä¸­äºŒ", "é˜´é˜³æ€ªæ°”", "ç§¯æå‘ä¸Š", "å¿§éƒæ‚²ä¼¤", "æ­£å¸¸"],
            value="æ­£å¸¸",
            label="äººæ ¼é€‰æ‹©"
        )
        clear_btn = gr.Button("æ¸…ç©ºå¯¹è¯")
        voice=gr.Audio(label="è¯­éŸ³è¾“å…¥",type="numpy")
        save=gr.Button("å¯¼å‡ºå¯¹è¯")

    
    # äº‹ä»¶å¤„ç†
    msg.submit(
        respond,
        inputs=[msg, chatbot, temperature, max_length, personality],
        outputs=[msg, chatbot]
    )
    save.click(
        save_chat,
        inputs=chatbot,
        outputs=gr.Textbox(label="å¯¼å‡ºç»“æœ"),
        queue=False
    )
    clear_btn.click(
        clear_memory,
        inputs=None,
        outputs=chatbot,
        queue=False
    )
    voice.change(
        inputs=process_voice(voice),
        outputs=msg,
        queue=False
    )

demo.launch(inbrowser=True)